*Autogenerated README by chatgpt

# Per-Step-Pruning (PSP) Callback for Hugging Face Transformers

## Overview

**Per-Step-Pruning (PSP)** is a custom `TrainerCallback` for Hugging Face's `transformers` library designed to allow efficient training of large models on smaller devices or to accelerate training on standard hardware. It dynamically masks a fraction of the model's trainable parameters during each training step based on their magnitude (L2 norm). 

This makes it possible to:

- Train larger models on devices with limited memory.
- Speed up training without reducing model size.
- Experiment with gradient sparsity and selective parameter updates.

The callback works as follows:

1. At the start of each training step, it computes the L2 norm of all trainable parameters.
2. Sorts parameters by magnitude.
3. Masks a specified fraction of the largest parameters by zeroing their gradients.
4. At the end of the step, ensures masked gradients remain zero, while leaving the parameters themselves trainable.

## Features

- Dynamic per-step gradient masking.
- Configurable fraction of parameters to mask (`mask_fraction`).
- Helps reduce memory footprint and accelerate training.
- Fully compatible with Hugging Face `Trainer` API via callbacks.

## Installation

Requires:

- Python 3.8+
- PyTorch
- Hugging Face Transformers

Install dependencies:

```bash
pip install torch transformers
````

## Usage

### Step 1: Import and Initialize PSP Callback

```python
from transformers import Trainer
from psp_callback import psp_callback  # assuming saved in psp_callback.py

psp = psp_callback(mask_fraction=0.5)
```

`mask_fraction` controls the fraction of top-norm parameters to mask per step. For example, `mask_fraction=0.5` masks 50% of the largest parameters by magnitude.

### Step 2: Add Callback to Trainer

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=50
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[psp]
)
```

### Step 3: Train the Model

```python
trainer.train()
```

The callback prints the number of masked layers at each step for monitoring.

## Use Cases

* **Train Large Models on Small Devices:** Reduce memory usage by masking gradients dynamically.
* **Accelerate Training:** By masking gradients of large parameters, compute overhead is reduced per step.
* **Experiment with Gradient Sparsity:** Useful for research on selective parameter updates and pruning strategies.

## Notes

* Only gradients are masked per step; the actual parameters remain trainable.
* `mask_fraction` should be chosen carefully; masking too many parameters may prevent the model from learning.
* Frozen parameters (`requires_grad=False`) are ignored.

```
```
